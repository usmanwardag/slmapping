\documentclass[a4paper, 11pt, oneside]{Thesis}  % Use the "Thesis" style, based on the ECS Thesis style by Steve Gunn
\graphicspath{Figures/}  % Location of the graphics files (set up for graphics to be in PDF format)

% Include any extra LaTeX packages required
%\usepackage[square, numbers, comma, sort&compress]{natbib}  % Use the "Natbib" style for the references in the Bibliography
\usepackage{verbatim}  % Needed for the "comment" environment to make LaTeX comments
\usepackage{vector}  % Allows "\bvec{}" and "\buvec{}" for "blackboard" style bold vectors in maths

\newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\hypersetup{urlcolor=blue, colorlinks=true}  % Colours hyperlinks in blue, but this can be distracting if there are many links.
%% ----------------------------------------------------------------
\begin{document}
% Set up the Title Page
\title  {Understanding Football from a Tactical Perspective}

% Include the chapters of the thesis, as separate files
% Just uncomment the lines as you write the chapters

\mainmatter	  % Begin normal, numeric (1,2,3...) page numbering
\pagestyle{fancy}  % Return the page headers back to the "fancy" style

% Include the chapters of the thesis, as separate files
% Just uncomment the lines as you write the chapters

%\tableofcontents  % Write out the Table of Contents

The goal is to create word form representations that meet the following two properties:

\begin{itemize}
\item They should account for phenomena arising from orthographic similarity between words such as stability, edge effects, transposed letter effects, and relative position effects.
\item They should be arranged in a low dimensional space such that distance between any two words indicate their orthographic similarity.
\end{itemize}

We consider two approaches: spatial coding and alphabetic representation. Spatial coding meets the first property in that effects like transposed letter, stability and relative positions are accounted for. However, there is no sense of distance between two word represenations in such a space, which makes it difficult to explore statistics and carry out tasks like word clustering. In alphabetic representation (for 3 letter words), each word is assigned a 3D point based on the three alphabets that form the word. Although there is a sense of distance between 3D points, such a represenation does not meet the first property. For example, words \textit{dog} and \textit{dig} and words \textit{dog} and \textit{dug} should be equally similar, but are not. Furthermore, it is challenging to represent variable length words in such a space. 

\textbf{The approach proposed here is to learn a 3D space representation by iterating over orthographic similarities between words.} \textit{The motivating idea is that the transitive property holds for words' orthographic similarity.} That is, if \(w_1\) is similar to \(w_2\) and \(w_2\) is similar to \(w_3\), then \(w_1\) is also similar to \(w_3\).

Consider a training sequence of \(L\) words given by \(W\). Let \(f\) is an operator that determines orthographic similarity between words (such operators are widely available in literature). Also let \(N\) is the dimension of the transformed representation (assumed to be 3 for our discussion). \textbf{The algorithm works as follows:}

\begin{itemize}
\item STEP 1: Place the first word at the origin of \(N\) dimensional space.
\item STEP 2: For every new word \(w\), compute the orthographic similarity with last \(N\) words and locate the position of \(w\) by satisfying the whole euclidean distance constraints.

\begin{equation}
{\sqrt{(x_w - x_{w-1})^2 + (y_w - y_{w-1})^2 + (z_w - z_{w-1})^2} = f(w, w-1)}
\end{equation}
\begin{equation}
{\sqrt{(x_w - x_{w-2})^2 + (y_w - y_{w-2})^2 + (z_w - z_{w-2})^2} = f(w, w-2)}
\end{equation}
\begin{equation}
{\sqrt{(x_w - x_{w-3})^2 + (y_w - y_{w-3})^2 + (z_w - z_{w-3})^2} = f(w, w-3)}
\end{equation}

\noindent Here, \(x_w\), \(y_w\) and \(z_w\) are the three unknowns which can be determined with three equations. Note that there are \(N\) equations for an N-dimensional space.

\item STEP 3: This step is applied prior to step 2. The last \(N\) words picked should be most similar. Here, similarity is defined as the number of same letters in the two words.
\end{itemize}

After repeating the process for the complete training sequence, the whole lexicon can be represented in a low dimensional space that meets the two properties defined at the beginning of the document.

\end{document}
